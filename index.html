<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>The Second Perception Test Challenge - ECCV Workshop 2024</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons -->
  <link href="img/fav.png" rel="icon">
  <link href="img/fav.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Poppins:300,400,500,700" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">

  <!-- =======================================================
    Theme Name: Regna
    Theme URL: https://bootstrapmade.com/regna-bootstrap-onepage-template/
    Author: BootstrapMade.com
    License: https://bootstrapmade.com/license/
  ======================================================= -->
</head>

<body>

  <!--==========================
  Header
  ============================-->
  <header id="header">
    <div class="container">

<!--       <div class="pull-left">
        <h1><a href="#hero">Perception Test</a></h1>
      </div> -->

      <nav id="nav-menu-container">
        <ul class="nav-menu">
          <li class="menu-active"><a href="#hero">Home</a></li>
          <li><a href="#about">Overview</a></li>
          <li><a href="#challenge">Challenge</a></li>
          <li><a href="#speakers">Speakers</a></li>
          <li><a href="#organizers">Organizers</a></li>
          <li><a href="https://ptchallenge-workshop.github.io/terms.html">Terms</a></li>
          <li><a href="https://ptchallenge-workshop.github.io/explore.html">Dataset Explorer</a></li>
          <li><a href="https://ptchallenge-workshop.github.io/challenge2023.html">Previous Challenges</a></li>
          <li><a href="https://github.com/deepmind/perception_test"><i class="fa fa-github" style="font-size:28px"></i></a></li>
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->


    <!--==========================
      Hero Section
    ============================-->
    <section id="hero">
      <div class="hero-container">
        <h1>The Second Perception Test Challenge</h1>
        <h2>Workshop at ECCV 2024, September 29, AM, Room: Suite 7</h2>
        <!-- <a href="#about" class="btn-get-started">Get Started</a> -->
      </div>
    </section><!-- #hero -->

<p></p>
	<p></p>
	
    <!--==========================
      About Us Section
    ============================-->
    <div class="container">
      <div class="row about-container">
        <!-- <div class="row justify-content-center"> -->

        <div class="col-lg-12 content order-lg-9 order-2">
          <div class="video-container">
              <!-- Video 1 -->
              <video controls autoplay muted loop id="videoId">
                  <source src="media/vis.mp4" type="video/mp4">
                  Your browser does not support the video tag.
              </video>
              <video controls autoplay muted loop id="videoId">
                  <source src="media/vis_2.mp4" type="video/mp4">
                  Your browser does not support the video tag.
              </video>
          </div>
    
    <section id="about">
             <h2 class="cta-title"><b>Overview</b></h2> 
            <p>Following the successful <a href="https://ptchallenge-workshop.github.io/challenge2023.html" target="_blank">2023 iteration</a>, we organise the second Perception Test Challenge with the goal of benchmarking multimodal perception models on the Perception Test (<a href="https://www.deepmind.com/blog/measuring-perception-in-ai-models" target="_blank">blog</a>, <a href="https://github.com/deepmind/perception_test" target="_blank">github</a>) - a diagnostic benchmark created by Google DeepMind to comprehensively probe the abilities of multimodal models across:
		    <ul>
			    <li>three modalities: video, audio, and text</li> 
			    <li>four skill areas: Memory, Abstraction, Physics, Semantics</li>
			    <li>four types of reasoning: Descriptive, Explanatory, Predictive, Counterfactual</li>
			    <li>six computational tasks: multiple-choice video-QA, grounded video-QA, object tracking, point tracking, action localisation, sound localisation</p>
		    </ul>
            <p>You can try yourself the Perception Test <a href="https://docs.google.com/forms/d/e/1FAIpQLScp49reYMAByszH6vo_y6umlkBPwsua2-kMpGjff3IV0YzYkw/viewform?usp=sf_link" target="_blank"><b>here</b></a>. </p>
            <p>Check the <a href="https://github.com/deepmind/perception_test" target="_blank">Perception Test github repo</a> for details about the data and annotations format, baselines, and metrics.</p>
            <p>Check the <a href="https://computerperception.github.io/" target="_blank">Computer Perception workshop at ECCV2022</a> for recorded talks and slides introducing the Perception Test benchmark.</p>
	    <p>Check the <a href="https://ptchallenge-workshop.github.io/challenge2023.html" target="_blank">First Perception Test challenge</a> for details of the previous challenge.</p>
	    <p></p>
	    <p><b>Contact: viorica at google.com, perception-test at google.com</b></p>
          <!-- </div> -->
          </div>
        </div>

      </div>
    </section><!-- #about -->

    <section id="team">
    <div id="challenge" class="container wow fadeInUp">
      <div class="section-header">
         <h2 class="cta-title"><b>Challenge</b></h2> 
	   <p>The Second Perception Test challenge includes the 6 original Perception Test tasks, plus an additional task focused on hour-long videoQA. (Check the links to access the eval.ai challenge pages)</p>   
           <ul>
		   <li><a href="https://eval.ai/web/challenges/challenge-page/2309/overview" target="_blank">Single object tracking</a>
<li><a href="https://eval.ai/web/challenges/challenge-page/2317/overview" target="_blank">Single point tracking</a>
<li><a href="https://eval.ai/web/challenges/challenge-page/2315/overview" target="_blank">Temporal action localisation</a>
<li><a href="https://eval.ai/web/challenges/challenge-page/2316/overview" target="_blank">Temporal sound localisation</a>
<li><a href="https://eval.ai/web/challenges/challenge-page/2308/overview" target="_blank">Multiple choice video question-answering</a>
<li><a href="https://eval.ai/web/challenges/challenge-page/2321/overview" target="_blank">Grounded video question-answering</a>
<li><a href="https://eval.ai/web/challenges/challenge-page/2330/overview" target="_blank">Hour-long videoQA</a>
	   </ul>
	      
	      <p>We offer cash prizes totalling EUR 20K to top competitors across tasks, with special awards for models that complete multiple/all tasks under zero-shot evaluation regime.</p>

           <h4><b>Timeline</b></h4>
           <ul>
             <li><b>June 10th, 2024</b>: Challenge server goes live with data from the validation split 
               <li><b> July 1st, 2024</b>: Held-out test split released 
                 <li> <b> September 14th, 2024</b>: Deadline for submissions
                   <li> <b> September 22nd, 2024 </b>: Winners announced
                     <li><b>September 29th, 2024</b>: Challenge-workshop at ECCV2024, Milan
           </ul>


<h4><b>The Second Perception Test Challenge winners</b></h4>
<p>We received 680 submissions from 123 teams across all seven tracks. We awarded runner-up and best performance per track.</p>
<p><b>Single Object Tracking</b></p>
		 <ul>
	<li>Best performance: Team <b>NJUST-THU</b> (Zhiqiang Zhong, Yang Yang, Fengqiang Wan, Henglu Wei, Xiangyang Ji) [<a href="https://drive.google.com/file/d/1JlSBBkGHs-vKTRm4h1nyU0YCur80zJV1/view?usp=sharing" target="_blank">report</a>]
	<li>Runner-up: Team <b>FAUgeddaboudit</b> (Amin Heydarshahi, Shubhaankar Gupta, Bernhard Egger) [<a href="https://drive.google.com/file/d/1hQE1qR6qkjZfY3EBK-8lM-DKqEW-hxjn/view?usp=sharing" target="_blank">report</a>]
</ul>
		 <p><b>Single Point Tracking</b></p>
		 <ul>
	<li>Best performance: Team <b>SV</b> (Hengzhi Zhang from Ricoh Software Research Center Beijing Co., Ltd) [<a href="https://drive.google.com/file/d/1v_tY4HABZw1UYkXgafdRdcpMevvh5jqa/view?usp=sharing" target="_blank">report</a>] 
		<li>Runner-up: Team <b>NJUST_kmg</b> (Yuxuan Zhang, Pengsong Niu, Kun Yu, Qingguo Chen, Yang Yang) [<a href="https://drive.google.com/file/d/15F6voPEcQL4j7e3I1omwqGMM8Z7Y4W5u/view?usp=sharing" target="_blank">report</a>]
		</ul>
		 <p><b>Temporal Action Localisation</b></p>
		 <ul>
			<li>Best performance: Team <b>NJUST--_KMG</b> (Yinan Han, Qingyuan Jiang, Hongming Mei, Yang Yang, Jinhui Tang) [<a href="https://drive.google.com/file/d/1HUkZKKojKoZhfRoDuepLZyUcY0b7SE5y/view?usp=sharing" target="_blank">report</a>]
		<li>Runner-up: Team <b>AITC</b> (Songlian Li, Zitao Gao, Huili Huang, Xinlong Sun) [<a href="https://drive.google.com/file/d/1nnkpe_MF4L1Bd6akkofiVL7mBRi7VcqX/view?usp=sharing" target="_blank">report</a>]
		</ul>
		 <p><b>Temporal Sound Localisation</b></p>
		 <ul>
	<li>Best performance: Team <b>NJUST_KMG0</b> (Haowei Gu, Weihao Zhu, Yang Yang) [<a href="https://drive.google.com/file/d/1Zxy32uKC6u9C9hOj-U9weNY3ZX5ji87w/view?usp=sharing" target="_blank">report</a>]
		<li>Runner-up: Team <b>JNU-Boat</b> (Linze Li, Rongchang Li, Cong Wu, Tianyang Xu, Xiao-Jun Wu, Josef Kittler) [<a href="https://drive.google.com/file/d/1V5rgm2QUw2_EGLXVSbsh6JEpXEhkx5dx/view?usp=sharing" target="_blank">report</a>]
		</ul>
		 <p><b>Multiple-Choice Video Question-Answering</b></p>
		 <ul>
			 <li>Best performance: Team <b>SEU-2023</b> (Yingzhe Peng, Yixiao Yuan, Zitian Ao, Huapeng Zhou, Kangqi Wang, Qipeng Zhu, Xu Yang) [<a href="https://drive.google.com/file/d/14exflxMQaTljThgT2F9I3KZm4fi9GMXt/view?usp=sharing" target="_blank">report</a>]
			 <li>Runner-up: Team <b>TTgogogo</b> (Dongshuai Li, Xingxian Liu, Fuyu Lv) [<a href="https://drive.google.com/file/d/1m1k5xNRF0eLbIFNS3zafD2P6puZgYP6t/view?usp=sharing" target="_blank">report</a>]
			 </ul>
		 <p><b>Grounded Video Question-Answering</b></p>
		 <ul>
	<li>Best performance: Team <b>Research newbie</b> (Yi-Jing Wu, Jo-Ting Chen, Hsing-Chen Lee, Jun-Cheng Chen) [<a href="https://drive.google.com/file/d/1Di-9VoMKvaeMV2z3LbGRyFv2phMfFnK_/view?usp=sharing" target="_blank">report</a>]
		<li>Runner-up: Team <b>UCF_CRCV</b>(Joseph Fioresi, Tina Tran, Mubarak Shah)[<a href="https://drive.google.com/file/d/1fyCgX-rUMu-fkJJKCFVF-qe3pOAACu5m/view?usp=sharing" target="_blank">report</a>]
		</ul>
		 <p><b>Hour-Long Video Question-Answering</b></p>
		 <ul>
			 <li>  Best performance: Team <b>blackmonkey</b> (Bozheng Li, Yangguang Ji, Yongliang Wu, Jiawang Cao, Wenbo Zhu, Jay Wu, Xu Yang) [<a href="https://drive.google.com/file/d/1R0zpIGfviujEjo7deg5IqmJQaPLxHs5J/view?usp=sharing" target="_blank">report</a>]
		<li> Runner-up: Team <b>JJ_James</b> (Yi Lu, Licheng Tang, Yuyang Sun, Wenyu Zhang, Weiheng Chi, Yalun Dai, Jing Wang) [<a href="https://drive.google.com/file/d/159OnxMooosm8gXLMql8F_OnacmNg1XOS/view?usp=sharing" target="_blank">report</a>]		
</ul>
	      </div>
         </div> 
    </div>
    </div>    
    </section>  



    <!--==========================
      Speaker Section
    ============================-->
    <section id="team">
      <div id="speakers" class="container wow fadeInUp">
        <div class="section-header">
          <h2 class="cta-title"><b>Workshop</b></h2> 
        </div>
			
        <div>
          <h4><b>Agenda (Room: Suite 7)</b></h4>
          <ul>
              <li>09:30 - 09:45 Welcome and introduction</li>
              <li>09:45 - 10:15 Overview of Perception Test</li>
              <li>10:15 - 10:45 Keynote: Abhinav Gupta</li>
	      <li>10:45 - 11:15 Coffee break</li>
	      <li>11:15 - 11:30 Challenges overview and winner announcement</li>
              <li>11:30 - 12:00 Oral presentations from the challenge winners</li>
              <li>12:00 - 12:30 Keynote: Josh Tenenbaum</li>
              <li>12:30 - 13:00 Roundtable and closing notes</li>
          </ul>
        </div>
        
        <h4><b>Speakers</b></h4>
        <div class="row" style="margin: auto;">
          <div class="col-lg-3 col-md-6">
           <div id="logo-holder">  
           <div class="member">
              <a href="https://www.cs.cmu.edu/~abhinavg/">
                <div class="pic"><img src="img/speakers/abhinav.jpeg" width="300" alt=""></div>
                <h4>Abhinav Gupta</h4>
                <span>Carnegie Mellon University</span>
              <div class="text">
               Associate Professor at Carnegie Mellon University, focused on scaling up learning by building self-supervised, lifelong and interactive learning systems. 
              </div>
	      </a>
            </div>
          </div>
          </div>
          
          <div class="col-lg-3 col-md-6">
           <div id="logo-holder">  
           <div class="member">
              <a href="https://web.mit.edu/cocosci/josh.html">
                <div class="pic"><img src="img/speakers/josh.jpeg" width="300" alt=""></div>
                <h4>Josh Tenenbaum</h4>
                <span>MIT</span>
              <div class="text">
               Professor at MIT, focusing on the computational basis of human cognition: learning concepts, judging similarity, inferring causal connections, forming perceptual representations, and more. 
             </div>
	      </a>
            </div>
          </div>
          </div>

      </div>
    </section><!-- #speaker -->


    <section id="team">

      <div id="organizers" class="container wow fadeInUp">
        <div class="section-header">
          <h2 class="cta-title"><b>Organizers</b></h2>

        </div>
        
       <div class="row">  

	       <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <div class="pic"><img src="img/organizers/joe.jpg" alt=""></div>
              <h4>Joe Heyward</h4>
              <span>Google Deepmind</span>
              <div class="text">
                Research Engineer at Google Deepmind. <br><b> Research:</b> computer vision.
              </div>
            </div>
            </div>
            </div>
            
            <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <div class="pic"><img src="img/organizers/joao.jpg" alt=""></div>
              <h4>Joao Carreira</h4>
              <span>Google Deepmind</span>
              <div class="text">
                Research Scientist at Google DeepMind. <br><b> Research:</b> video processing, general perception systems.
              </div>
            </div>
            </div>
            </div>
            
            <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <div class="pic"><img src="img/organizers/dima.jpg" alt=""></div>
              <h4>Dima Damen</h4>
              <span>Bristol University</span>
              <div class="text">
                Professor of Computer Vision. <br><b> Research:</b> computer vision, video understanding, perception benchmarks.
              </div>
            </div>
            </div>
            </div>
            
            <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <div class="pic"><img src="img/organizers/andrew.jpg" alt=""></div>
              <h4>Andrew Zisserman</h4>
              <span>University of Oxford</span>
              <div class="text">
                Professor of Computer Vision Engineering at Oxford and a Royal Society Research Professor. <br><b> Research:</b> computer vision, machine learning.
              </div>
            </div>
            </div>
            </div>

	       <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <div class="pic"><img src="img/organizers/viorica.jpg" alt=""></div>
              <h4>Viorica Pa&#774;tra&#774;ucean</h4>
              <span>Google Deepmind</span>
              <div class="text">
                Research Scientist at Google DeepMind. <br><b> Research:</b> computer vision, scalable learning, biologically plausible learning.
              </div>
            </div>
            </div>
            </div>
            
       </div>

      </div>
    </section><!-- #team -->


  </main>

  <!--==========================
    Footer
  ============================-->
  <footer id="footer">
    <div class="footer-top">
      <div class="container">

      </div>
    </div>

    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong>SecondPerceptionTestChallenge</strong>. All Rights Reserved
      </div>
      <div class="credits">
        <!--
          All the links in the footer should remain intact.
          You can delete the links only if you purchased the pro version.
          Licensing information: https://bootstrapmade.com/license/
          Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=Regna
        -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- #footer -->

  <a href="#" class="back-to-top"><i class="fa fa-chevron-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/waypoints/waypoints.min.js"></script>
  <script src="lib/counterup/counterup.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>

  <!-- Contact Form JavaScript File -->
  <script src="contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>

</body>
</html>
